@inproceedings{Snir1995MPI,
  title={MPI: The Complete Reference},
  author={Snir, Marc and Otto, Steve W and Walker, David W and Dongarra, Jack and Huss-Lederman, Steven},
  pages={4038-4040},
  year={1995},
 keywords={CiteSeerX;citations;MPI: The Complete Reference;M Snir;S Otto;S Huss-Lederman;D Walker;J Dongarra},
 abstract={Gropp, William D; Huss-Lederman, Steven; Lumsdaine, Andrew; Lusk, Ewing; Nitzberg, Bill; Snir, Marc},
}

@book{Geist2011PVM,
  title={PVM (Parallel Virtual Machine)},
  author={Geist, Al},
  pages={19-23},
  year={2011},
 keywords={中国;信息技术;软件产业;国际化},
 abstract={Synonyms Message passing Definition Parallel Virtual Machine (PVM) is a software package that permits a heterogeneous collection of Unix and/or Windows computers hooked together by a network to be used as a single large parallel computer. Thus large computational problems can be solved more cost-effectively by using the aggregate power and memory of many computers. The software is very robust and portable. The source, which is available free from the PVM website, has been compiled on everything from laptops to CRAYs. Because PVM supports fault tolerance and dynamic adaptability, hundreds of sites around the world continue using PVM to solve important scientific, industrial, and medical problems. PVM has also been popular as an educational tool to teach parallel programming because of its simple intuitive user interface. Discussion Introduction PVM (Parallel Virtual Machine) is often lumped together with the Message Passing Interface (MPI) standard, because PVM was ...},
}

@inproceedings{Shvachko2010The,
  title={The Hadoop Distributed File System},
  author={Shvachko, Konstantin and Kuang, Hairong and Radia, Sanjay and Chansler, Robert},
  booktitle={IEEE Symposium on Mass Storage Systems \& Technologies},
  pages={1-10},
  year={2010},
 keywords={Internet;distributed databases;network operating systems;Hadoop distributed file system;Yahoo;data storage;data stream;enterprise data;Bandwidth;Clustering algorithms},
 abstract={The Hadoop Distributed File System (HDFS) is designed to store very large data sets reliably, and to stream those data sets at high bandwidth to user applications. In a large cluster, thousands of servers both host directly attached storage and execute user application tasks. By distributing storage and computation across many servers, the resource can grow with demand while remaining economical at every size. We describe the architecture of HDFS and report on experience using HDFS to manage 25 petabytes of enterprise data at Yahoo!.},
}

@inproceedings{Dean2004MapReduce,
  title={MapReduce: Simplified Data Processing on Large Clusters.},
  author={Dean, Jeffrey and Ghemawat, Sanjay},
  booktitle={Conference on Symposium on Opearting Systems Design \& Implementation},
  pages={137-150},
  year={2004},
 keywords={requirements modelling;design;refinement;transformation},
 abstract={Abstract MapReduce is a programming model and an associated implementation for processing and generating large datasets that is amenable to a broad variety of real-world tasks. Users specify the computation in terms of a map and a reduce function, and the underlying runtime system automatically parallelizes the computation across large-scale clusters of machines, handles machine failures, and schedules inter-machine communication to make efficient use of the network and disks. Programmers find the system easy to use: more than ten thousand distinct MapReduce programs have been implemented internally at Google over the past four years, and an average of one hundred thousand MapReduce jobs are executed on Google's clusters every day, processing a total of more than twenty petabytes of data per day.},
}

@article{Lars2011HBase,
  title={HBase},
  author={Lars, George},
  year={2011},
 abstract={If you're looking for a scalable storage solution to accommodate a virtually endless amount of data, this book shows you how Apache HBase can fulfill your needs. As the open source implementation of Google's BigTable architecture, HBase scales to billions of rows and millions of columns, while ensuring that write and read performance remain constant. Many IT executives are asking pointed questions about HBase. This book provides meaningful answers, whether you're evaluating this non-relational database or planning to put it into practice right away. Discover how tight integration with Hadoop  Lars, George},
}

@book{Lyubimov2016Apache,
  title={Apache Mahout: Beyond MapReduce},
  author={Lyubimov, Dmitriy and Palumbo, Andrew},
  publisher={CreateSpace Independent Publishing Platform},
  year={2016},
}

@inproceedings{Vavilapalli2013Apache,
  title={Apache Hadoop YARN: yet another resource negotiator},
  author={Vavilapalli, Vinod Kumar and Murthy, Arun C and Douglas, Chris and Agarwal, Sharad and Konar, Mahadev and Evans, Robert and Graves, Thomas and Lowe, Jason and Shah, Hitesh and Seth, Siddharth},
  booktitle={Symposium on Cloud Computing},
  pages={5},
  year={2013},
 abstract={The initial design of Apache Hadoop [1] was tightly focused on running massive, MapReduce jobs to process a web crawl. For increasingly diverse companies, Hadoop has become the data and computational agor\&\#225;---the de facto place where data and computational resources are shared and accessed. This broad adoption and ubiquitous usage has stretched the initial design well beyond its intended target, exposing two key shortcomings: 1) tight coupling of a specific programming model with the resource management infrastructure, forcing developers to abuse the MapReduce programming model, and 2) centralized handling of jobs' control flow, which resulted in endless scalability concerns for the scheduler. In this paper, we summarize the design, development, and current state of deployment of the next generation of Hadoop's compute platform: YARN. The new architecture we introduced decouples the programming model from the resource management infrastructure, and delegates many scheduling functions (e.g., task fault-tolerance) to per-application components. We provide experimental evidence demonstrating the improvements we made, confirm improved efficiency by reporting the experience of running YARN on production environments (including 100% of Yahoo! grids), and confirm the flexibility claims by discussing the porting of several programming frameworks onto YARN viz. Dryad, Giraph, Hoya, Hadoop MapReduce, REEF, Spark, Storm, Tez.},
}

@article{2015Aliyun,
  title={Aliyun, An Alibaba Unit, Is Building China's First "Cloud Hospital"},
  author={Aliyun},
  journal={Flare},
  year={2015},
 abstract={The article focuses on the partnership of Aliyun, the subsidiary of cloud computing company Alibaba, with Xi'An International Medical Investment and DHC Software for the creation of a hospital administration platform in China.},
}

@book{Varia2011Best,
  title={Best Practices in Architecting Cloud Applications in the AWS Cloud},
  author={Varia, Jinesh},
  pages={457-490},
  year={2011},
 keywords={applications ‐ best practices in architecting cloud applications in the AWS cloud;elasticity, one of the fundamental properties of cloud ‐ elasticity, power to scale computing resources up and down easily and with minimal friction;decoupling components using queues},
 abstract={Abstract IntroductionBackground Cloud ConceptsCloud Best PracticesGrepTheWeb Case StudyFuture Research DirectionsConclusion AcknowledgmentsReferences},
}

@phdthesis{徐明2014基于,
  title={基于Hadoop的空间数据挖掘研究},
  author={徐明},
  school={陕西师范大学},
  year={2014},
 keywords={面元加权Voronoi图;K-Means空间聚类;Hadoop;MapReduce;空间数据挖掘},
 abstract={空间数据挖掘是对空间数据中的隐含知识、空间关系自动提取的研究方法。空间数据既可以是点、线、面等空间实体数据,又可以是具有地理位置和属性特征的空间对象,数据类型繁多,使得单一的空间数据挖掘技术难以满足实际应用需求。并且数据量庞大,使得空间数据挖掘极为耗时,不能有效地满足其时效性需求。常见的空间数据挖掘技术包括统计分析方法、聚类分析方法、空间分析方法、计算几何方法等。 计算几何中的Voronoi图方法能较好地表达空间实体的邻近关系,可以处理点、线、面等空间实体数据挖掘问题。而目前线、面等复杂实体目标的加权Voronoi图研究较少且效率较低,因此针对面元目标的加权Voronoi图研究具有重要价值。 空间聚类方法是常用的空间数据挖掘技术,尤其是K-Means空间聚类方法,能够处理具有地理位置和属性特征的空间对象。但随着信息化社会的发展,空间数据呈爆炸式的增长,而串行算法计算效率不高,难以处理海量空间数据。 鉴于Hadoop在处理大规模海量数据上的优势,本文对面元加权Voronoi图和K-Means空间聚类算法进行了MapReduce并行化设计,主要的工作成果总结如下: (1)对Hadoop相关技术进行了阐述,分析了分布式文件系统HDFS的工作机制和MapReduce的执行流程,为后续算法设计提供了理论依据。 (2)针对具有较为复杂拓扑结构的面状空间数据,结合面元边界提取思想,改进了面元加权Voronoi图算法,并在Hadoop平台上实现了其并行化。并利用面元加权Voronoi图来解决空间数据挖掘中的空间目标影响范围界定问题。 (3)针对具有地理位置和属性特征双重含义的空间数据,设计实现了基于Hadoop的K-Means空间聚类的并行化算法,并以新浪微博用户数据为例进行用户聚类,验证了算法的有效性和可行性。实现了基于Google Map的聚类可视化。},
}

@phdthesis{马磊2016一种基于,
  title={一种基于HDFS的分布式多级R树空间索引研究},
  author={马磊},
  school={中国测绘科学研究院},
  year={2016},
  address={北京},
 keywords={空间索引;并行查询;分布式文件管理系统},
 abstract={随着测绘技术不断的发展,测绘地理信息部门拥有的地理数据飞速增长,而相应的空间数据存储与处理仍然较多使用传统的方法,己不能支持海量空间数据。随着计算机技术的不断发展,数据库技术以及文件组织方式发生了巨大的变化。如今云计算、分布式处理及并行网格计算技术的逐渐成熟,在各行各业有着广泛的应用。HDFS (Hadoop Distributed File System)以及MapReduce作为目前广泛应用的分布式存储与计算框架,支持利用计算机集群进行海量数据的存储与快速处理,这为提高海量矢量数据的存储与计算提供了新途径。因此面向日益增长的大数据集管理要求,将分布式技术引入空间数据存储与组织中是空间数据存储与处理方法研究的重要课题之一。本文通过将传统空间数据存储以及查询方法与HDFS分布式存储技术相结合,借助于可扩展的分布式文件系统HDFS存储空间数据,解决了海量空间数据的存储问题。由于传统的空间索引并不能很好的适用于分布式空间数据存储结构,因此本文设计了全局索引与局部索引相结合的分布式多级R树空间索引——DMLR(Distributed Multi-layer R Tree)。DMLR空间索引采用STR叶结点分割思想对空间数据进行分区,可以更好的应对数据分布不均衡的情况。利用Spark并行计算框架对DMLR索引进行并行构建,加快了DMLR索引的构建效率。此外通过将传统的空间数据查询方法与并行计算技术相结合,设计了基于DMLR空间索引的空间数据并行查询方法。包括并行范围聚集查询、并行k邻近查询、并行空间连接查询等方法。DMLR空间索引为分布式空间数据索引提供了一种新方法。通过空间数据并行查询实验也验证了DMLR索引在分布式环境下对海量空间数据管理的有效性。},
}

@article{方金云2015基于,
  title={基于Spark的空间数据实时访存技术的研究},
  author={方金云 and 刘羽 and 姚晓 and 陈翠婷 and 张梦菲 and 肖茁建 and 张广发},
  journal={地理信息世界},
  volume={22},
  number={6},
  pages={24-31},
  year={2015},
 keywords={实时空间信息服务;并行分析算法;地理信息系统},
 abstract={研究并实现了基于Spark的空间查询算法.根据空间查询特性和Spark分布式内存计算模型,设计了HBase分布式存储、分布式空间索引、Spark分布式内存计算框架的空间区域查询算法和Spark Streaming 的空间查询算法,提供实时在线空间查询服务.实验表明,基于Spark streaming并行空间查询算法*可以提供空间数据的实时空间查询服务.},
}

@article{温馨2015基于,
  title={基于Shark/Spark的分布式空间数据分析框架},
  author={温馨 and 罗侃 and 陈荣国},
  journal={地球信息科学学报},
  volume={17},
  number={4},
  pages={401-407},
  year={2015},
 keywords={Shark;Spark;Hadoop;空间数据库;空间查询},
 abstract={随着空间数据的与日俱增,传统依托于单节点的空间数据管理方法,已难以满足海量数据高并发的需求。云计算的兴起带来机遇与挑战,分布式技术与数据库技术的优势互补,为云计算下高效的数据管理提供了可能。本文提出一种在分布式计算引擎(Shark/Spark)中集合之关键技术(包括空间数据映射、空间数据加载、数据备份及空间查询等),将空间数据库对空间数据的高效存储、索引及查询优势与分布式计算引擎对复杂计算的优势相结合,实现一种基于Shark/Spark的分布式空间数据分析框架。在具体实现中,通过空间自定义函数和空间函数下推2种方式实现空间查询,结果表明,影响返回结果数据量的空间查询更适合下推给空间数据库完成,而不影响返回结果数据量的空间查询,利用分布式计算引擎直接运算更有优势。同时,通过与现有的一种分布式GIS方案(Arc GIS on Hadoop)对比发现,空间数据库的空间索引可有效提高查询效率,空间数据管理也更加独立。},
}

@article{李璐明2015基于弹性分布数据集的海量空间数据密度聚类,
  title={基于弹性分布数据集的海量空间数据密度聚类},
  author={李璐明 and 蒋新华 and 廖律超},
  journal={湖南大学学报(自科版)},
  volume={42},
  number={8},
  pages={116-124},
  year={2015},
 keywords={空间数据;聚类算法;弹性分布式数据集;SPARK},
 abstract={摘　要: 为了快速挖掘大规模空间数据的聚集特性,在cluster_dp密度聚类算法基础上,提出了一种基于弹性分布数据集的并行密度聚类方法 PClusterdp.首先,设计一种能平衡工作负载弹性分布数据集分区方法,根据数据在空间的分布情况,自动划分网格并分配数据,使得网格内数据量相对均衡,达到平衡运算节点负载的目的;接着,提出一种适用于并行计算的局部密度定义,并改进聚类中心的计算方式,解决了原始算法需要通过绘制决策图判断聚类中心对象的缺陷;最后,通过网格内及网格间聚簇合并等优化策略,实现了大规模空间数据的快速聚类处理.实验结果表明,借助Spark数据处理平台编程实现算法,本方法可以有效实现大规模空间数据的快速聚类,与传统的密度聚类方法相比具有较高的精确度与更好的系统处理性能.},
}

@article{靳凤营2016基于,
  title={基于 Spark 的土地利用矢量数据空间叠加分析方法},
  author={靳凤营 and 张丰 and 杜震洪 and 刘仁义 and 李荣亚},
  journal={浙江大学学报(理学版)},
  volume={43},
  number={1},
  pages={40-44},
  year={2016},
 keywords={Spark;土地利用;叠加分析;矢量数据},
 abstract={针对海量土地利用矢量数据空间叠加分析的效率问题,提出了基于Spark的土地利用矢量数据空间叠加分析方法,通过弹性分布式数据集实现索引过滤与叠加计算,为解决空间叠加分析的瓶颈问题做了新的尝试.实验结果表明,相较基于Oracle数据管理的叠加分析方法,该方法显著提高了空间叠加分析效率,更适合面向海量土地利用矢量数据的叠加分析.},
}

@article{Abouzeid2009HadoopDB,
  title={HadoopDB: an architectural hybrid of MapReduce and DBMS technologies for analytical workloads},
  author={Abouzeid, Azza and Bajda-Pawlikowski, Kamil and Abadi, Daniel and Silberschatz, Avi and Rasin, Alexander},
  journal={Proceedings of the Vldb Endowment},
  volume={2},
  number={1},
  pages={922-933},
  year={2009},
 keywords={CiteSeerX;citations;HadoopDB: An Architectual Hybrid of MapReduce and DBMS Technologies for Analytical Workloads;A Abouzeid;K Pawlikowski;D Abadi;A Silberschatz;A Rasin},
 abstract={ABSTRACT  ABSTRACT The production environment,for analytical data management,ap- plications is rapidly changing. Many enterprises are shifting away from deploying their analytical databases on high-end proprietary machines, and moving towards cheaper, lower-end, commodity hardware, typically arranged in a shared-nothing MPP architecture, often in a virtualized environment,inside public or private 鈥渃louds鈥. At the same time, the amount of data that needs to be analyzed is exploding, requiring hundreds to thousands of machines to work in parallel to perform the analysis. There tend to be two schools of thought regarding what tech- nology to use for data analysis in such an environment.,Propo- nents of parallel databases argue that the strong emphasis,on per- formance,and efficiency of parallel databases makes,them,well- suited to perform such analysis. On the other hand, others argue that MapReduce-based systems are better suited due to their supe- rior scalability, fault tolerance, and flexibility to handle unstructured data. In this paper, we explore the feasibility of building a hybrid system that takes the best features from both technologies; the pro- totype we built approaches parallel databases in performance,and efficiency, yet still yields the scalability, fault tolerance, and flexi- bility of MapReduce-based systems.},
}

@inproceedings{Witayangkurn2012Performance,
  title={Performance comparisons of spatial data processing techniques for a large scale mobile phone dataset},
  author={Witayangkurn, Apichon and Horanont, Teerayut and Shibasaki, Ryosuke},
  booktitle={Com.Geo},
  pages={1-6},
  year={2012},
 keywords={GPS;Hadoop;cloud computing;mobile phone;spatial query},
 abstract={Mobile technology, especially mobile phone, is very popular nowadays. Increasing number of mobile users and availability of GPS-embedded mobile phones generate large amount of GPS trajectories that can be used in various research areas such as people mobility and transportation planning. However, how to handle such a large-scale dataset is a significant issue particularly in spatial analysis domain. In this paper, we aimed to explore a suitable way for extracting geo-location of GPS coordinate that achieve large-scale support, fast processing, and easily scalable both in storage and calculation speed. Geo-locations are cities, zones, or any interesting points. Our dataset is GPS trajectories of 1.5 million individual mobile phone users in Japan accumulated for one year. The total number was approximately 9.2 billion records. Therefore, we conducted performance comparisons of various methods for processing spatial data, particularly for a huge dataset. In this work, we first processed data on PostgreSQL with PostGIS that is a traditional way for spatial data processing. Second, we used java application with spatial library called Java Topology suite (JTS). Third, we tried on Hadoop Cloud Computing Platform focusing on using Hive on top of Hadoop to allow SQL-like support. However, Hadoop/Hive did not support spatial query at the moment. Hence, we proposed a solution to enable spatial support on Hive. As the results, Hadoop/hive with spatial support performed best result in large-scale processing among evaluated methods and in addition, we recommended techniques in Hadoop/Hive for processing different types of spatial data.},
}

@article{ESRIGIS,
  title={GIS Tools for Hadoop: Big Spatial Data Analytics for the Hadoop Framework},
  author={ESRI},
  year={2014},
 abstract={Enabling Hadoop to include spatial data and spatial analysis is the goal of this Esri Open Source effort.},
}

@inproceedings{Eldawy2016SpatialHadoop,
  title={SpatialHadoop: A MapReduce framework for spatial data},
  author={Eldawy, Ahmed and Mokbel, Mohamed F},
  booktitle={IEEE  International Conference on Data Engineering},
  pages={1352-1363},
  year={2016},
 keywords={data handling;spatial data structures;MapReduce framework;R-tree;SpatialFileSplitter;SpatialHadoop;SpatialRecordReader;expressive high level language;kNN;language layer},
 abstract={There has been a recent explosion in the amounts of spatial data produced by several devices such as smart phones, satellites, space telescopes, medical devices, among others. This variety of such spatial data makes it widely used across important applications such as brain simulations, identifying cancer clusters, tracking infectious disease, drug addiction, simulating climate changes, and event detection and analysis. While there are several distributed systems that are designed to handle Big Data in general, e.g., Hadoop, Hive, Spark, and Impala, they all fall short in supporting spatial data efficiently. As a result, there are great research efforts in either extending these systems or building new systems to efficiently support Big Spatial Data.},
}

@inproceedings{Yu2015GeoSpark,
  title={GeoSpark: a cluster computing framework for processing large-scale spatial data},
  author={Yu, Jia and Wu, Jinxuan and Sarwat, Mohamed},
  booktitle={The  Sigspatial International Conference},
  pages={1-4},
  year={2015},
 keywords={cluster computing;large-scale data;spatial data},
 abstract={This paper introduces GeoSpark an in-memory cluster computing framework for processing large-scale spatial data. GeoSpark consists of three layers: Apache Spark Layer, Spatial RDD Layer and Spatial Query Processing Layer. Apache Spark Layer provides basic Spark functionalities that include loading / storing data to disk as well as regular RDD operations. Spatial RDD Layer consists of three novel Spatial Resilient Distributed Datasets (SRDDs) which extend regular Apache Spark RDDs to support geometrical and spatial objects. GeoSpark provides a geometrical operations library that accesses Spatial RDDs to perform basic geometrical operations (e.g., Overlap, Intersect). System users can leverage the newly defined SRDDs to effectively develop spatial data processing programs in Spark. The Spatial Query Processing Layer efficiently executes spatial query processing algorithms (e.g., Spatial Range, Join, KNN query) on SRDDs. GeoSpark also allows users to create a spatial index (e.g., R-tree, Quad-tree) that boosts spatial data processing performance in each SRDD partition. Preliminary experiments show that GeoSpark achieves better run time performance than its Hadoop-based counterparts (e.g., SpatialHadoop).},
}

@inproceedings{Oriani2012From,
  title={From Backup to Hot Standby: High Availability for HDFS},
  author={Oriani, André and Garcia, Islene C.},
  booktitle={IEEE  Symposium on Reliable Distributed Systems},
  pages={131-140},
  year={2012},
 keywords={Hadoop;HDFS;distributed file systems;high availability;fault tolerance;replication;failover},
 abstract={Cluster-based distributed file systems generally have a single master to service clients and manage the namespace. Although simple and efficient, that design compromises availability, because the failure of the master takes the entire system down. Before version 2.0.0-alpha, the Hadoop Distributed File System (HDFS) -- an open-source storage, widely used by applications that operate over large datasets, such as MapReduce, and for which an uptime of 24x7 is becoming essential -- was an example of such systems. Given that scenario, this paper proposes a hot standby for the master of HDFS achieved by (i) extending the master's state replication performed by its check pointer helper, the Backup Node, and by (ii) introducing an automatic fail over mechanism. The step (i) took advantage of the message duplication technique developed by other high availability solution for HDFS named Avatar Nodes. The step (ii) employed another Hadoop software: ZooKeeper, a distributed coordination service. That approach resulted in small code changes, 1373 lines, not requiring external components to the Hadoop project. Thus, easing the maintenance and deployment of the file system. Compared to HDFS 0.21, tests showed that both in loads dominated by metadata operations or I/O operations, the reduction of data throughput is no more than 15% on average, and the time to switch the hot standby to active is less than 100 ms. Those results demonstrate the applicability of our solution to real systems. We also present related work on high availability for other file systems and HDFS, including the official solution, recently included in HDFS 2.0.0-alpha.},
}

@article{SinghMapReduce,
  title={MapReduce WordCount: Execution and Effects of Altering Parameters},
  author={Singh, A. J and Sarjolta, Vibha},
 abstract={ABSTRACT: With the growing advent of new technologies, devices and communication means, a large amount of data is being generated by everything around us that forms a part of what today is known as Big Data. In order to process such colossal quantities of data Apache's Hadoop acts as a software that deals with this type of huge and unstructured data with efficiency. In this paper one of the basic programs of Hadoop, that is, MapReduce WordCount is executed in a single node setup. The changes in the size of input files and the number of reduce tasks affecting the execution time of the program is studied. This paper aims at comparing the execution time of WordCount under varying conditions.},
}

@inproceedings{Grolinger2014Challenges,
  title={Challenges for MapReduce in Big Data},
  author={Grolinger, Katarina and Hayes, Michael and Higashino, Wilson A. and L'Heureux, Alexandra and Allison, David S. and Capretz, Miriam A. M.},
  booktitle={Proc. of the IEEE  World Congress on Services},
  pages={182-189},
  year={2014},
 keywords={Big Data;SQL;data analysis;data privacy;learning (artificial intelligence;parallel programming;relational databases;security of data;storage management;Big Data analytics},
 abstract={Abstract In the Big Data community, MapReduce has been seen as one of the key enabling approaches for meeting continuously increasing demands on computing resources imposed by massive data sets. The reason for this is the high scalability of the MapReduce paradigm which allows for massively parallel and distributed execution over a large number of computing nodes. This paper identifies MapReduce issues and challenges in handling Big Data with the objective of providing an overview of the field, facilitating better planning and management of Big Data projects, and identifying opportunities for future research in this field. The identified challenges are grouped into four main categories corresponding to Big Data tasks types: data storage (relational databases and NoSQL stores), Big Data analytics (machine learning and interactive analytics), online processing, and security and privacy. Moreover, current efforts aimed at improving and extending MapReduce to address identified challenges are presented. Consequently, by identifying issues and challenges MapReduce faces when handling Big Data, this study encourages future Big Data research.},
}

@book{Guller2015Spark,
  title={Spark SQL},
  author={Guller, Mohammed},
  publisher={Apress},
  year={2015},
 abstract={Abstract Ease of use is one of the reasons Spark became popular. It provides a simpler programming model than Hadoop MapReduce for processing big data. However, the number of people who are fluent in the languages supported by the Spark core API is a lot smaller than the number of people who know the venerable SQL.},
}

@article{MllibMLlib,
  title={MLlib: Scalable Machine Learning on Spark},
  author={Mllib, What Is},
 abstract={MLlib: Scalable Machine Learning on SparkXiangrui Meng 1Collaborators: Ameet Talwalkar, Evan Sparks, Virginia Smith, Xinghao Pan, Shivaram Venkataraman, Matei Zaharia, Rean Griffith, John Duchi, Joseph Gonzalez, Michael Franklin, Michael I. Jordan, Tim Kraska, etc. What is MLlib?2What is MLlib?MLlib is a Spark subproject providing machine learning primitives:• initial contribution from AMPLab, UC Berkeley • shipped with Spark since version 0.8 • 33 contributors3What is MLlib?• logistic regressio},
}

@inproceedings{Xin2013GraphX,
  title={GraphX: a resilient distributed graph system on Spark},
  author={Xin, Reynold S and Gonzalez, Joseph E and Franklin, Michael J and Stoica, Ion},
  booktitle={International Workshop on Graph Data Management Experiences and Systems},
  pages={2},
  year={2013},
 abstract={From social networks to targeted advertising, big graphs capture the structure in data and are central to recent advances in machine learning and data mining. Unfortunately, directly applying existing data-parallel tools to graph computation tasks can be cumbersome and inefficient. The need for intuitive, scalable tools for graph computation has lead to the development of new graph-parallel systems (e.g., Pregel, PowerGraph) which are designed to efficiently execute graph algorithms. Unfortunately, these new graph-parallel systems do not address the challenges of graph construction and transformation which are often just as problematic as the subsequent computation. Furthermore, existing graph-parallel systems provide limited fault-tolerance and support for interactive data mining. We introduce GraphX, which combines the advantages of both data-parallel and graph-parallel systems by efficiently expressing graph computation within the Spark data-parallel framework. We leverage new ideas in distributed graph representation to efficiently distribute graphs as tabular data-structures. Similarly, we leverage advances in data-flow systems to exploit in-memory computation and fault-tolerance. We provide powerful new operations to simplify graph construction and transformation. Using these primitives we implement the PowerGraph and Pregel abstractions in less than 20 lines of code. Finally, by exploiting the Scala foundation of Spark, we enable users to interactively load, transform, and compute on massive graphs.},
}

@book{Guller2015Sparks,
  title={Spark Streaming},
  author={Guller, Mohammed},
  publisher={Apress},
  year={2015},
 abstract={Abstract Batch processing of historical data was one of the first use cases for big data technologies such as Hadoop and Spark. In batch processing, data is collected for a period of time and processed in batches. A batch processing system processes data spanning from hours to years, depending on the requirements. For example, some organizations run nightly batch processing jobs, which process data collected throughout the day by various systems.},
}

@inproceedings{Zaharia2012Resilient,
  title={Resilient distributed datasets: a fault-tolerant abstraction for in-memory cluster computing},
  author={Zaharia, Matei and Chowdhury, Mosharaf and Das, Tathagata and Dave, Ankur and Ma, Justin and Mccauley, Murphy and Franklin, Michael J and Shenker, Scott and Stoica, Ion},
  booktitle={Usenix Conference on Networked Systems Design and Implementation},
  pages={2-2},
  year={2012},
 keywords={angiosarcoma;liver;tomography, x-ray},
 abstract={Abstract We present Resilient Distributed Datasets (RDDs), a distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a fault-tolerant manner. RDDs are motivated by two types of applications that current computing frameworks handle inefficiently: iterative algorithms and interactive data mining tools. In both cases, keeping data in memory can improve performance by an order of magnitude. To achieve fault tolerance efficiently, RDDs provide a restricted form of shared memory, based on coarse-grained transformations rather than fine-grained updates to shared state. However, we show that RDDs are expressive enough to capture a wide class of computations, including recent specialized programming models for iterative jobs, such as Pregel, and new applications that these models do not capture. We have implemented RDDs in a system called Spark, which we evaluate through a variety of user applications and benchmarks.},
}

@inproceedings{Ng1994Efficient,
  title={Efficient and Effective Clustering Methods for Spatial Data Mining},
  author={Ng, Raymond T and Han, Jiawei},
  booktitle={International Conference on Very Large Data Bases},
  pages={144-155},
  year={1994},
 keywords={CiteSeerX;Raymond T. Ng;Jiawei Han},
 abstract={Spatial data mining is the discovery of interesting relationships and characteristics that may exist implicitly in spatial databases. In this paper, we explore whether clustering methods have a role to play in spatial data mining. To this end, we develop a new clustering method called CLARANS which is based on randomized search. We also de- velop two spatial data mining algorithms that use CLARANS. Our analysis and experiments show that with the assistance of CLARANS, these two algorithms are very effective and can lead to discoveries that are difficult to find with current spatial data mining algorithms.},
}

@article{李德仁2002论空间数据挖掘和知识发现的理论与方法,
  title={论空间数据挖掘和知识发现的理论与方法},
  author={李德仁 and 王树良 and 李德毅 and 王新洲},
  journal={武汉大学学报(信息科学版)},
  volume={27},
  number={3},
  pages={221-233},
  year={2002},
 keywords={空间数据挖掘;知识发现;理论方法},
 abstract={首先分析了空间数据挖掘和知识发现(SDMKD)的内涵和外延;然后分别研究了用于SDMKD的概率论、证据理论、空间统计学、规则归纳、聚类分析、空间分析、模糊集、云理论、粗集、神经网络、遗传算法、可视化、决策树、空间在线数据挖掘等理论和方法及其进展;最后展望了SDMKD的发展前景.},
}

@phdthesis{杨春成2004空间数据挖掘中聚类分析算法的研究,
  title={空间数据挖掘中聚类分析算法的研究},
  author={杨春成},
  school={解放军信息工程大学},
  pages={61-62},
  year={2004},
  address={郑州},
 keywords={数据挖掘;聚类分析;聚类算法;面状地理实体;相似性;遗传算法;小波变换;空间数据;栅格数据},
 abstract={随着数据获取技术的发展以及数据获取手段的多样化,大量与空间位置相关的数据被收集,人们迫切需要强有力的数据分析工具来从这些数据中获取信息或知识,这一需求导致了空间数据挖掘这一全新研究领域的出现。 空间聚类分析既可以发现隐含在海量数据中的聚类规则,又可以与其它数据挖掘方法结合使用,发掘更深层次的知识,提高数据挖掘的效率和质量,是空间数据挖掘的重要研究方向之一。本文探讨了面状地理实体和栅格空间数据的聚类算法,主要研究工作如下: (1)系统归纳和总结了文献中发表的大量空间聚类算法,对各种算法的适用条件、性能、优缺点和计算复杂度进行了细致分析与比较。 (2)分析了面状地理实体的特点,针对面状地理实体提出了一种顾及几何形状相似性的简单多边形之间最近距离计算方法。该方法具有以下优点:①适用于简单多边形之间距离计算;②在得到多边形之间最近距离的同时,得到多边形之间彼此面对的线段链,为进一步求取多边形之间几何形状相似性奠定了基础。 (3)提出了基于遗传算法的面状地理实体划分聚类算法。算法在搜索过程中不需要其它外部信息,仅以适应度函数为依据,利用种群中每个个体的适应度来搜索最优解,具有普适性较好、聚类结果质量高的优点。 (4)针对聚类数未知条件下面状地理实体的聚类问题,提出了基于簇分解的面状地理实体聚类算法和基于聚类有效性函数的面状地理实体聚类算法。验证了两种算法的各项性能指标。 (5)提出了满足旋转与平移不变性的线段链相似性评价方法。该方法具有计算简便,不需要针对每对线段链进行优化计算的优点。依据该评价方法,设计了面状地理实体聚类算法CACSS。提出了综合考虑距离与几何形状相似性的面状地理实体相似性准则,依据该准则改写了基于遗传算法的面状地理实体聚类算法和CLARANS算法。 (6)将小波多分辨率分解和Kmeans算法有机结合,提出了一种既能改进算法效率,又能保证聚类结果质量的新聚类算法WaveKmeans。},
}

@article{曾玲2005关联规则在空间数据挖掘中的研究,
  title={关联规则在空间数据挖掘中的研究},
  author={曾玲 and 熊才权 and 胡恬},
  journal={计算机与数字工程},
  volume={33},
  number={6},
  pages={71-73},
  year={2005},
 keywords={关联规则;空间数据库;数据挖掘},
 abstract={在智能化、集成化的空间数据应用领域中,空间数据挖掘是一门很重 要的技术,而关联规则分析是空间数据挖掘的主要方法之一.文章基于数据挖掘中的关联规则分析方法,提出不同于一般数据挖掘的算法,设定兴趣度量,并通过将 项的概念泛化为空间谓词,事务的概念泛化为邻域,关联规则的概念泛化为同位规则,发现多种形式的有效规则,并用逻辑语言或类SQL语言方式描述规则,以使 空间数据挖掘趋于规范化和工程化.最后进行了实评.},
}

@article{Bychowski2003Open,
  title={Open GIS Consortium, Inc.},
  author={Bychowski, Contributors: Tom and NavTech and Williams, Jonathan and Hutchison and Niedzwiadek, Harry and Matters, Image and Bishr, Yaser and Matters, Image},
  number={March},
  year={2003},
 abstract={Copyright 1999, 2000 BBN TechnologiesCopyright 1999, 2000 Cadcorp Ltd.Copyright 1999, 2000 CubeWerx Inc.Copyright 1999, 2000 IONIC Software s.a.Copyright 1999, 2000 Laser-Scan LimitedCopyright 1999, 2000 SICAD Geomatics GmbH \& Co. oHGCopyright 1999, 2000 Social Change Online Pty LtdCopyright 1999, 2000 US Army Engineer Research and Development Center},
}

@article{Urbanek2012proj4,
  title={proj4: A simple interface to the PROJ.4 cartographic projections library},
  author={Urbanek, Simon},
  year={2012},
}

@article{Johansson2002Using,
  title={Using Java Topology Suite for real-time data generalisation and integration},
  author={Johansson, Mikael and Harrie, Lars},
  journal={Proceedings of the Workshop of the International Society for Photogrammetry \& Remote Sensing},
  year={2002},
 abstract={Abstract Introduction The amount of cartographic data distributed on the Internet is still increasing. Today, most of the data are raster data, but the emerging XML-standards will make it easier to distribute vector data on the Internet. One of the main advantages with vector data is that it is easier to integrate and generalise them than raster data. However, this requires a suitable technical environment to perform the data-integration and generalisation. During the last years some prototype systems for distributing vector data on the Internet have been developed. Many of these systems are based on the two XML-standards Geographic Markup Language (GML; OGC, 2002) and Scaleable Vector Graphics (SVG; W3C, 2002). These two standards are complementary: GML is used for storing and distributing geographic data and SVG is used for presenting data. In many current prototype applications the cartographic data is stored in a database; when a map request is performed to the database a GML-file is created. The GML-file is then translated into an SVG-file (in e.g. an XSLT-transformation). In this step it is also possible to perform some generalisation transformations (Lehto and Kilpel盲inen, 2000, 2001a, 2001b). The project described in this paper also follows this workflow. The new thing is the use of Java Topology Suite (JTS; Vivid Solutions, 2002; see Figure 1). The purpose of this step is to have a powerful and flexible environment to perform the data-integration and generalisation. Java Topology Suite is a class library in Java. JTS was chosen as environment for generalisation and data-integration mainly due to following reasons. Like GML, JTS conforms to the Simple Features Specification for SQL (OGC, 2002) and it contains robust implementations of the most fundamental spatial algorithms (in 2D). Furthermore, JTS is an open source and free to use and modify in research.},
}

@inproceedings{Guttman1984R,
  title={R-trees: a dynamic index structure for spatial searching},
  author={Guttman, Antonin},
  booktitle={ACM SIGMOD International Conference on Management of Data},
  pages={47-57},
  year={1984},
 keywords={Object recognition;Continuous transformation;Trace learning;Inferior temporal cortex;Invariant representations},
 abstract={In order to handle spatial data efficiently, as required in computer aided design and geo-data applications, a database system needs an index mechanism that will help it retrieve data items quickly according to their spatial locations However, traditional indexing methods are not well suited to data objects of non-zero size located m multi-dimensional spaces In this paper we describe a dynamic index structure called an R-tree which meets this need, and give algorithms for searching and updating it. We present the results of a series of tests which indicate that the structure performs well, and conclude that it is useful for current database systems in spatial applications},
}

@article{Huang2001Optimizing,
  title={Optimizing storage utilization in R-tree dynamic index structure for spatial databases ☆},
  author={Huang, P. W and Lin, P. L and Lin, H. Y},
  journal={Journal of Systems \& Software},
  volume={55},
  number={3},
  pages={291-299},
  year={2001},
 keywords={Spatial database;Window query;Spatial object;Compact R-tree;Storage utilization;Search performance},
 abstract={Spatial databases have been increasingly and widely used in recent years. The R-tree proposed by Guttman is probably the most popular dynamic index structure for efficiently retrieving objects from a spatial database according to their spatial locations. However, experiments show that only about 70% storage utilization can be achieved in Guttman's R-tree and its variants. In this paper, we propose a compact R-tree structure which can achieve almost 100% storage utilization. Our experiments also show that the search performance of compact R-trees is very competitive as compared to Guttman's R-trees. In addition, the overhead cost of building a compact R-tree is much lower than that of a Guttman's R-tree because the frequency of node splitting is reduced significantly.},
}

@article{谢俊平2012拓扑关系和方向关系的统一表达模型,
  title={拓扑关系和方向关系的统一表达模型},
  author={谢俊平 and 杨敏华},
  journal={测绘科学},
  volume={37},
  number={2},
  pages={150-152+158},
  year={2012},
 keywords={拓扑关系;方向关系;统一表达},
 abstract={统一表达空间关系中的拓扑关系和方向关系是非常有必要的.本文首 先对已有的方向模型进行分析和比较,提出了一种方向模型,即采用锥形的方法将空间参照对象的内部、边界和外部分别划分出9个方向区域,描述空间目标对象与 这些方向区域的交集的情况,然后结合这种方向模型和九交模型,进而提出了一种能统一表达拓扑关系和方向关系的形式化模型.},
}

@phdthesis{董亭亭2013大数据下空间数据索引和,
  title={大数据下空间数据索引和kNN查询技术的研究},
  author={董亭亭},
  school={大连理工大学},
  year={2013},
  address={大连},
 keywords={空间数据索引;kNN查询;倒排网格索引;移动互联网},
 abstract={随着移动互联网和物联网技术的广泛应用,空间位置信息数据量迅速 增长。而大规模的数据使得传统的空间数据索引和查询方法面临着新的挑战。例如,由于数据量的增长使得传统的内存式索引结构的磁盘访问次数大大增加。因此, 有效的空间数据查询问题需要新的可扩展的分布式索引结构。而目前常用的少数几种分布式索引为基于R-tree的分布式索引和基于Voronoi图的分布式 索引,这些索引结构仍存在一些不足。首先,由于R-tree的层次型结构不易分散化,使得该索引结构的可扩展性不高;基于Voronoi图的索引结构只适 合处理静态的数据集和查询点,当有数据点动态加入时...},
}

@article{Jacox2007Spatial,
  title={Spatial join techniques},
  author={Jacox, Edwin H and Samet, Hanan},
  journal={Acm Transactions on Database Systems},
  volume={32},
  number={1},
  pages={7},
  year={2007},
 keywords={External memory algorithms;plane-sweep;spatial join},
 abstract={A variety of techniques for performing a spatial join are reviewed. Instead of just summarizing the literature and presenting each technique in its entirety, distinct components of the different techniques are described and each is decomposed into an overall framework for performing a spatial join. A typical spatial join technique consists of the following components: partitioning the data, performing internal-memory spatial joins on subsets of the data, and checking if the full polygons intersect. Each technique is decomposed into these components and each component addressed in a separate section so as to compare and contrast similar aspects of each technique. The goal of this survey is to describe the algorithms within each component in detail, comparing and contrasting competing methods, thereby enabling further analysis and experimentation with each component and allowing the best algorithms for a particular situation to be built piecemeal, or, even better, enabling an optimizer to choose which algorithms to use.},
}

@article{Ghosh2017Install1,
  title={Install Apache Hadoop on Ubuntu on Single Cloud Server Instance},
  author={Ghosh, Abhishek},
  year={2017},
}

@article{Ghosh2017Install2,
  title={Install Apache Spark on Ubuntu Single Cloud Server With Hadoop},
  author={Ghosh, Abhishek},
  year={2017},
}

@book{Fern2016Automated,
  title={Automated Spark Clusters Deployment for Big Data with Standalone Applications Integration},
  author={Fernández, A. M. and Torres, J. F. and Troncoso, A. and Martínez-Álvarez, F.},
  pages={1-10},
  year={2016},
 keywords={Big data;Spark;Algorithms;Automated deployment},
 abstract={Abstract The huge amount of data stored nowadays has turned big data analytics into a very trendy research field. Spark has emerged as a very powerful and widely used paradigm for clusters deployment and big data management. However, to get started is still a very tough task, due to the excessive requisites that all nodes must fulfil. Thus, this work introduces a web service specifically designed for an easy and efficient Spark cluster management. In particular, a service with a friendly graphical user interface has been developed to automate the deploying of clusters. Another relevant feature is the possibility of integrating any algorithm into the web service. That is, the user only needs to provide the executable file and the number of required inputs for a proper parametrization. Finally, an illustrative case study is included to show ad hoc algorithms usage (the MLlib implementation for k-means, in this case) across the nodes of the configured cluster.},
}

@inproceedings{Zhao2016An,
  title={An adaptive tuning strategy on spark based on in-memory computation characteristics},
  author={Zhao, Y. and Hu, F. and Chen, H.},
  booktitle={International Conference on Advanced Communication Technology},
  pages={484-488},
  year={2016},
 keywords={Java;statistical analysis;storage management;Java;Kryo serialization algorithm;Spark application runtime statistics;Spark integrated serialization algorithm;adaptive tuning strategy;in-memory computation characteristic;round-robin structure},
 abstract={We present an adaptive tuning method to improve Spark performance, especially for its in-memory computation. This manner serves one purpose: making a better use of memory reasonably through adaptively adopting suitable category based on Spark application runtime statistics on different working sets. This solution works in two steps. Firstly, it collects run-time statistics dynamically and stores them in round-robin structures to save memory. Secondly, it can change system storage category based on these statistics. Additionally we focus on serialization strategy optimization. For this purpose we test Spark integrated serialization algorithms: Java and Kryo serialization algorithms, and make a comparison of their performance. In order to gain flexibility we change Spark serialization mechanism by setting the default serialization unit from one RDD to one block. In this way, for the case which RDD has huge amount of blocks our solution can use different serialization algorithms to serialize different blocks in one RDD. We show that our solution is expressive enough to obtain 2x speedup than original Spark when there is inadequate memory resource.},
}

@article{Haklay2008OpenStreetMap,
  title={OpenStreetMap: User-Generated Street Maps},
  author={Haklay, M and Weber, P},
  journal={Pervasive Computing IEEE},
  volume={7},
  number={4},
  pages={12-18},
  year={2008},
 keywords={Web sites;cartography;geographic information systems;groupware;software tools;OpenStreetMap project;Web site;Wikipedia;cartography;copyright scheme},
 abstract={The OpenStreetMap project is a knowledge collective that provides user-generated street maps. OSM follows the peer production model that created Wikipedia; its aim is to create a set of map data that's free to use, editable, and licensed under new copyright schemes. A considerable number of contributors edit the world map collaboratively using the OSM technical infrastructure, and a core group, estimated at approximately 40 volunteers, dedicate their time to creating and improving OSM's infrastructure, including maintaining the server, writing the core software that handles the transactions with the server, and creating cartographical outputs. There's also a growing community of software developers who develop software tools to make OSM data available for further use across different application domains, software platforms, and hardware devices. The OSM project's hub is the main OSM Web site.},
}

@article{赵捷2015基于新浪微博的数据挖掘及可视化研究,
  title={基于新浪微博的数据挖掘及可视化研究},
  author={赵捷 and 谭国强},
  journal={电子技术与软件工程},
  number={18},
  pages={181-182},
  year={2015},
 keywords={新浪微博;信息可视化;数据挖掘;人物分析},
 abstract={摘　要: 本文介绍一种具有实际应用意义的小型数据挖掘可视化系统。系统通过Http Client进行新浪微博模拟登录及信息获取,采用本地CSV格式文件储存数据并使用Hibernate实现与Mysql数据库连接,通过Java Web形式,运用D3.js及Echarts等数据可视化技术实现本地用户信息可视化的转化及浏览;同时提供了微博大V推荐查看来增加系统的趣味性,支持本地用户查看、网络用户的信息在线挖掘及分析展示,经反复测试、分析和对比,证明了本系统的实用性和娱乐趣味性。},
}

@article{廉捷2011新浪微博数据挖掘方案,
  title={新浪微博数据挖掘方案},
  author={廉捷 and 周欣 and 曹伟 and 刘云},
  journal={清华大学学报自然科学版},
  number={10},
  pages={1300-1305},
  year={2011},
 keywords={新浪微博;新浪API;数据检索;网页解析},
 abstract={摘　要: 随着新浪微博用户群体的增长,新浪微博的数据获取是微博研究首先需要解决的问题。该文提出了基于新浪微博API与基于页面解析的新浪微博数据获取方案。程序逻辑控制API调用方法与频率,获取JSON对象并解析实现高效数据获取。同时将传统的网络爬虫结合网页解析技术结合API同时使用,解决了因API接口开放不完善,且因在返回结果数量上限与调用频率方面的限制,导致不能有效实现新浪微博数据的全面获取的问题。经过实验测试,通过2套方案的结合可以实现新浪微博数据高效全面的获取。},
}

@article{Nemeslaki2011Web,
  title={Web crawler research methodology},
  author={Nemeslaki, András and Pocsarovszky, Károly},
  journal={General Information},
  year={2011},
 keywords={e-business research;web search;web crawler;Hungarian web;social network analyis},
 abstract={Abstract In economic and social sciences it is crucial to test theoretical models against reliable and big enough databases. The general research challenge is to build up a well-structured database that suits well to the given research question and that is cost efficient at the same time. In this paper we focus on crawler programs that proved to be an effective tool of data base building in very different problem settings. First we explain how crawler programs work and illustrate a complex research process mapping business relationships using social media information sources. In this case we illustrate how search robots can be used to collect data for mapping complex network relationship to characterize business relationships in a well defined environment. After that extend the case and present a framework of three structurally different research models where crawler programs can be applied successfully: exploration, classification and time series analysis. In the case of exploration we present findings about the Hungarian web agency industry when no previous statistical data was available about their operations. For classification we show how the top visited Hungarian web domains can be divided into predefined categories of e-business models. In the third research we used a crawler to gather the values of concrete pre-defined records containing ticket prices of low cost airlines from one single site. Based on the experiences we highlight some conceptual conclusions and opportunities of crawler based research in e-business. --},
}

@phdthesis{罗一纾2013微博爬虫的相关技术研究,
  title={微博爬虫的相关技术研究},
  author={罗一纾},
  school={哈尔滨工业大学},
  year={2013},
 keywords={社交媒体;微博爬虫;去重策略;分布式数据存储技术},
 abstract={社交媒体作为web2.0时代的标志,提供了以用户为中心的各种交流模式和途径。用户在社交媒体上发表和传播消息,关注自己感兴趣的人物。社交媒体中一般拥有数以亿计的人物节点,他们之间通过关注和粉丝关系连成了巨大的社会网络,消息通过这张巨大的社会网络传播。大部分社交媒体提供API以便获取社交媒体数据进行相关研究,但是由于API的相关限制造成数据获取困难,直接造成研究无法进行。所以对于社交媒体爬虫的相关技术研究具有重大意义。　　本文主要以国内微博媒体为研究对象,主要针对新浪微博,研究微博爬虫的相关技术。本文研究的技术包括:爬行策略...},
}

@article{Recordon2011The,
  title={The OAuth 2.0 Authorization Protocol},
  author={Recordon, David and Hardt, Dick and Hammerlahav, Eran},
  journal={International Journal of Pharmaceutics},
  volume={271},
  number={1–2},
  pages={95-113},
  year={2011},
 keywords={AZT zidovudine which is chemically 3′-azido-3′-deoxythymidine or azidothymidine [CAS-30516-87-1;zidovudine which is chemically 3′-azido-3′-deoxythymidine or azidothymidine [CAS-30516-87-1;ASP ascorbyl palmitate;ascorbyl palmitate;CHOL cholesterol;cholesterol;DCP dicetyl-phosphate;dicetyl-phosphate;KI potassium Iodide;potassium Iodide},
 abstract={Transdermal permeation of aspasomal AZT, ASP-AZT aqueous dispersion and AZT-solution across excised rat skin was investigated in vitro using Franz diffusion cell. Permeation of aspasomal AZT was much higher than the other two preparations. However, ASP-AZT aqueous dispersion has also enhanced permeation of AZT significantly over the AZT-solution, indicating skin permeation enhancing property of ascorbyl palmitate.},
}

@article{Yang1999Map,
  title={Map Projection Transformation},
  author={Yang, Qihe H and Snyder, John P and Tobler, Waldo R},
  journal={Taylor \& Francis Ltd London},
  year={1999},
 keywords={CiteSeerX;citations;Motivation in learning contexts: Theoretical advances and methodological implications;S E Volet;S Järvelä},
 abstract={CiteSeerX - Scientific documents that cite the following paper: Motivation in learning contexts: Theoretical advances and methodological implications},
}

@article{韩华瑞2016湖北省微博签到活动空间差异分析——以新浪微博为例,
  title={湖北省微博签到活动空间差异分析——以新浪微博为例},
  author={韩华瑞 and 代侦勇},
  journal={测绘与空间地理信息},
  number={10},
  pages={159-162},
  year={2016},
 keywords={湖北省;微博签到活动;空间差异},
 abstract={摘　要: 选取了湖北省2014年1—11月的微博签到数据,通过对55933个POI、5820136次微博签到量进行空间统计分析发现:湖北省微博签到次数呈现出明显的空间差异,17个地区中武汉市微博签到量所占百分比高达71.19%,在湖北省中占绝对优势,其他16个地区中宜昌、襄阳、荆州三地区所占百分比为13.83%,其余13个地区仅占14.98%。在17个地级市中又以各市中心的签到量最大。将17个地级市微博签到量与各地区2014年GDP做散点图。发现GDP与签到量之间呈正相关,通过皮尔森相关系数进行验证,在0.01的显著水平下皮尔森系数为0.959,呈现出高度相关,说明微博签到量与经济发展水平关系密切。},
}

@inproceedings{Ye2011Exploiting,
  title={Exploiting geographical influence for collaborative point-of-interest recommendation},
  author={Ye, Mao and Yin, Peifeng and Lee, Wang Chien and Lee, Dik Lun},
  booktitle={Proceeding of the  International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2011, Beijing, China, July},
  pages={325-334},
  year={2011},
 keywords={collaborative recommendation;geographical influence;location-based social networks},
 abstract={In this paper, we aim to provide a point-of-interests (POI) recommendation service for the rapid growing location-based social networks (LBSNs), e.g., Foursquare, Whrrl, etc. Our idea is to explore user preference, social influence and geographical influence for POI recommendations. In addition to deriving user preference based on user-based collaborative filtering and exploring social influence from friends, we put a special emphasis on geographical influence due to the spatial clustering phenomenon exhibited in user check-in activities of LBSNs. We argue that the geographical influence among POIs plays an important role in user check-in behaviors and model it by power law distribution. Accordingly, we develop a collaborative recommendation algorithm based on geographical influence based on naive Bayesian. Furthermore, we propose a unified POI recommendation framework, which fuses user preference to a POI with social influence and geographical influence. Finally, we conduct a comprehensive performance evaluation over two large-scale datasets collected from Foursquare and Whrrl. Experimental results with these real datasets show that the unified collaborative recommendation approach significantly outperforms a wide spectrum of alternative recommendation approaches.},
}

@article{朱晨曦2016基于微博签到的地理空间信息研究,
  title={基于微博签到的地理空间信息研究},
  author={朱晨曦 and 晏王波},
  journal={地理空间信息},
  volume={14},
  number={5},
  pages={28-30},
  year={2016},
 keywords={空间分析;热点分析;VGI;南京市},
 abstract={摘　要: 随着Web2.0 时代的来临,志愿者地理服务(VGI)逐渐改变了传统的地理信息服务模式.以新浪微博为例,从网络社会空间入手,实现了数据的获取、清洗、可视化及空间分析,对南京市热点地区进行了研究.研究表明,南京市辖区的鼓楼、建邺、秦淮等老城区和浦口、江宁、栖霞等地区分别由于商业聚集地吸引大量人流和大学城聚集成为整个城市的热点地区,集聚程度高,人流量相对较大.该模式为城市应急、城市规划、基础设施建设提供了依据,也为VGI 模式下的地理空间信息研究提供了可能.},
}

@article{张子昂2015基于微博签到数据的景区旅游活动时空行为特征研究——以南京钟山风景名胜区为例,
  title={基于微博签到数据的景区旅游活动时空行为特征研究——以南京钟山风景名胜区为例},
  author={张子昂 and 黄震方 and 靳诚 and 关健 and 曹芳东},
  journal={地理与地理信息科学},
  volume={31},
  number={4},
  pages={121-126},
  year={2015},
 keywords={新浪微博;数据挖掘;时空特征;钟山风景名胜区},
 abstract={随着社交网络的普及与信息通信技术的发展,以微博为代表的社交网络位置服务信息逐渐增多,为研究旅游活动行为特征提供了新的数据来源和视角.该文以新浪微博为研究对象,引入LBS签到数据,通过划分性别与地域属性,利用“横向”、“纵向”时间分层法,对南京钟山风景名胜区内部游客活动时空演变特征进行实证研究.发现性别活动特征在虚拟社交网络与实体地理空间中差异显著,女性游客现实生活中的社会制约没能反映在虚拟网络空间中;本、外地游客在景区内部活动特征也表现出明显的差异性,外地游客相比本地游客在景区中活动时间变化规律更加明显;此外发现景区内非景点休闲区是虚拟网络空间重要的组成部分,能够间接反映游客在景区的游览活动特征.},
}

@inproceedings{Agrawal1995Mining,
  title={Mining sequential patterns},
  author={Agrawal, R and Srikant, R},
  booktitle={Eleventh International Conference on Data Engineering},
  pages={3-14},
  year={1995},
 keywords={retail data processing;knowledge acquisition;pattern matching;very large databases;customer transactions;large database;customer-ID;transaction time;sequential pattern mining;algorithms},
 abstract={We are given a large database of customer transactions, where each transaction consists of customer-id, transaction time, and the items bought in the transaction. We introduce the problem of mining sequential patterns over such databases. We present three algorithms to solve this problem, and empirically evaluate their performance using synthetic data. Two of the proposed algorithms, AprioriSome and AprioriAll, have comparable performance, albeit AprioriSome performs a little better when the minimum number of customers that must support a sequential pattern is low. Scale-up experiments show that both AprioriSome and AprioriAll scale linearly with the number of customer transactions. They also have excellent scale-up properties with respect to the number of transactions per customer and the number of items in a transaction.},
}

@misc{Agrawal1999Automatic,
  title={Automatic subspace clustering of high dimensional data for data mining applications},
  author={Agrawal, Rakesh and Gehrke, Johannes Ernst and Gunopulos, Dimitrios and Raghavan, Prabhakar},
  pages={94--105},
  year={1999},
 keywords={subspace clustering;clustering;dimensionality reduction},
 abstract={A method for finding clusters of units in high-dimensional data having the steps of determining dense units in selected subspaces within a data space of the high-dimensional data, determining each cluster of dense units that are connected to other dense units in the selected subspaces within the data space, determining maximal regions covering each cluster of connected dense units, determining a minimal cover for each cluster of connected dense units, and identifying the minimal cover for each cluster of connected dense units.},
}

@article{王佐成2006空间关联规则的双向挖掘,
  title={空间关联规则的双向挖掘},
  author={王佐成 and 汪林林 and 薛丽霞 and 李永树},
  journal={计算机科学},
  volume={33},
  number={7},
  pages={199-203},
  year={2006},
 keywords={数据挖掘;空间数据;关联规则;双向挖掘},
 abstract={空间数据库中关联规则挖掘不仅需要考虑关系元组属性之间的关系 --纵向关系,更需要挖掘元组之间的关系--横向关系,如相邻、相交、重叠等.本文通过分析空间数据库的存储模式,借鉴事务数据库关联规则的挖掘方法,对 空间关联规则进行完整定义,并对规则的兴趣度度量进行探讨.根据挖掘的方向将空间数据挖掘归纳为纵向挖掘、横向挖掘、双向挖掘.在双向挖掘中,提出一种新 算法,该算法根据挖掘任务进行约束,缩小挖掘空间,然后通过空间计算将空间关系转化为非空间关系,经过多次循环,获取非空间项集,进而挖掘出空间关联规 则.据此提出空间数据双向挖掘工作流程,并通过实例进行了验证.},
}

@book{李德仁2013空间数据挖掘理论与应用,
  title={空间数据挖掘理论与应用},
  author={李德仁},
  publisher={科学出版社},
  year={2013},
 keywords={空间信息系统},
 address={北京},
 abstract={本书提出数据场、云模型、地学粗空间和空间数据挖掘视角,构建空间数据挖掘金字塔,给出挖掘机理,介绍概念格,总结空间数据源,导出清理空间观测数据的选权迭代法。面向GIS数据研究空间的关联规则、分布规则、概化规则和聚类规则的挖掘。},
}

@article{陈江平2004空间关联规则挖掘算法研究,
  title={空间关联规则挖掘算法研究},
  author={陈江平},
  journal={计算机工程},
  volume={30},
  number={23},
  pages={53-55},
  year={2004},
 keywords={数据挖掘;空间关联规则;元模式;多概念层次},
 abstract={提出了多层次空间关联规则的定义后,利用多层次的空间概念关系,给出了一种基于元模式的多层次空间关联规则挖掘算法AP_MLSAM;在此算法中,预先确定用户感兴趣的规则模式与对象,然后分别在该对象的各数据层上进行大项集的计数,最后得到与用户给定的元模式形式一致的空间关联规则。实验表明,算法是有效的。},
}

@article{蔡伟杰2001关联规则挖掘综述,
  title={关联规则挖掘综述},
  author={蔡伟杰 and 张晓辉 and 朱建秋 and 朱扬勇},
  journal={计算机工程},
  volume={27},
  number={5},
  pages={31-33},
  year={2001},
 keywords={数据挖掘;关联规则;频集;OLAP},
 abstract={介绍了关联规则挖掘的研究性况,提出了关联规则的分类方法,对一些典型算法进行了分析和秤价,指出传统关系规则衡量标准的不足,归纳出关联规则的价值衡量方,展望了关联规则挖掘的未来研究方向。},
}

@inproceedings{Koperski1995Discovery,
  title={Discovery of Spatial Association Rules in Geographic Information Databases},
  author={Koperski, Krzysztof and Han, Jiawei},
  booktitle={International Symposium on Advances in Spatial Databases},
  pages={47-66},
  year={1995},
}

@inproceedings{Ester1999Spatial,
  title={Spatial Data Mining: A Database Approach},
  author={Ester, Martin and Kriegel, Hans Peter and Sander, J},
  booktitle={International Symposium on Advances in Spatial Databases},
  pages={47-66},
  year={1999},
 keywords={Spatial Data Mining;Neighborhood Graphs;Efficient Query Processing},
 abstract={Abstract . Knowledge discovery in databases (KDD) is an important task in spatial databases since both, the number and the size of such databases are rapidly growing. This paper introduces a set of basic operations which should be supported by a spatial database system (SDBS) to express algorithms for KDD in SDBS. For this purpose, we introduce the concepts of neighborhood graphs and paths and a small set of operations for their manipulation. We argue that these operations are sufficient for KDD algorithms considering spatial neighborhood relations by presenting the implementation of four typical spatial KDD algorithms based on the proposed operations. Furthermore, the efficient support of operations on large neighborhood graphs and on large sets of neighborhood paths by the SDBS is discussed. Neighborhood indices are introduced to materialize selected neighborhood graphs in order to speed up the processing of the proposed operations. Keywords: Spatial Data Mining, Neighborhood Graphs, Effici...},
}

@inproceedings{万幼2008k,
  title={k-邻近空间关系下的同位关联模式挖掘},
  author={万幼 and 边馥苓},
  booktitle={地理信息系统全国博士生学术论坛},
  year={2008},
  address={南京},
 keywords={空间对象;关联规则挖掘;空间同位模式;k-邻近空间;感兴趣特征},
 abstract={空间对象具有自相关、连续性、多尺度等特点,导致空间关联规则挖掘与传统的关联规则挖掘不同,不存在统计的"事务",挖掘更加复杂。空间同位模式是存在于地理空间中频繁的且紧密相临的空间要素的集合。根据空间自相关性,感兴趣的特征很可能在紧邻的区域同时存在,因此空间同位才是真正想要考察的。},
}

@article{Bian2009A,
  title={A Novel Spatial Co-location Pattern Mining Algorithm Based on k-Nearest Feature Relationship},
  author={Bian, Fuling},
  journal={Geomatics \& Information Science of Wuhan University},
  volume={34},
  number={3},
  pages={331-334},
  year={2009},
 keywords={k-nearest feature;spatial co-location pattern;KNFCOM;spatial association rule},
 abstract={We define a k-nearest feature based on co-location patterns,and develop k-nearest feature co-location mining(KNFCOM) algorithm to mine this kind of co-location patterns.The experimental results show that KNFCOM algorithm is efficient and scalable for mining spatial co-location patterns from various large spatial datasets.},
}

@article{Jin2006A,
  title={A Joinless Approach for Mining Spatial Colocation Patterns},
  author={Jin, Soung Yoo and Shekhar, Shashi},
  journal={IEEE Transactions on Knowledge \& Data Engineering},
  volume={18},
  number={10},
  pages={1323-1337},
  year={2006},
 keywords={Spatial data mining;Spatial data mining;association rule;colocation pattern;spatial neighbor relationship;association rule;colocation pattern;spatial neighbor relationship},
 abstract={Spatial colocations represent the subsets of features which are frequently located together in geographic space. Colocation pattern discovery presents challenges since spatial objects are embedded in a continuous space, whereas classical data is often discrete. A large fraction of the computation time is devoted to identifying the instances of colocation patterns. We propose a novel joinless approach for efficient colocation pattern mining. The joinless colocation mining algorithm uses an instance-lookup scheme instead of an expensive spatial or instance join operation for identifying colocation instances. We prove the joinless algorithm is correct and complete in finding colocation rules. We also describe a partial join approach for spatial data which are clustered in neighborhood areas. We provide the algebraic cost models to characterize the performance dominance zones of the joinless method and the partial join method with a current join-based colocation mining method, and compare their computational complexities. In the experimental evaluation, using synthetic and real-world data sets, our methods performed more efficiently than the join-based method and show more scalability in dense data.},
}

@article{Huang2004Discovering,
  title={Discovering Colocation Patterns from Spatial Data Sets: A General Approach},
  author={Huang, Yan and Shekhar, Shashi and Xiong, Hui},
  journal={IEEE Transactions on Knowledge \& Data Engineering},
  volume={16},
  number={12},
  pages={1472-1485},
  year={2004},
 keywords={65;Index Terms- Colocation patterns;Index Terms- Colocation patterns;spatial association rules;participation index;participation index;spatial association rules},
 abstract={Given a collection of Boolean spatial features, the colocation pattern discovery process finds the subsets of features frequently located together. For example, the analysis of an ecology data set may reveal symbiotic species. The spatial colocation rule problem is different from the association rule problem since there is no natural notion of transactions in spatial data sets which are embedded in continuous geographic space. In this paper, we provide a transaction-free approach to mine colocation patterns by using the concept of proximity neighborhood. A new interest measure, a participation index, is also proposed for spatial colocation patterns. The participation index is used as the measure of prevalence of a colocation for two reasons. First, this measure is closely related to the {\rm{cross}}{\hbox{-}}K function, which is often used as a statistical measure of interaction among pairs of spatial features. Second, it also possesses an antimonotone property which can be exploited for computational efficiency. Furthermore, we design an algorithm to discover colocation patterns. This algorithm includes a novel multiresolution pruning technique. Finally, experimental results are provided to show the strength of the algorithm and design decisions related to performance tuning.},
}

@article{Wang2009Efficient,
  title={Efficient Discovery of Spatial CoLocation Patterns Using the iCPI-tree},
  author={Wang, Lizhen and Bao, Yuzhen and Lu, Zhongyu},
  journal={Open Information Systems Journal},
  volume={3},
  number={2},
  pages={69-80},
  year={2009},
 abstract={Abstract With the rapid growth and extensive applications of the spatial dataset, it's getting more important to solve how to find spatial knowledge automatically from spatial datasets. Spatial co-location patterns represent the subsets of features whose instances are frequently located together in geographic space. It's difficult to discovery co-location patterns be-cause of the huge amount of data brought by the instances of spatial features. A large fraction of the computation time is devoted to identifying the table instances of co-location patterns. The essence of co-location patterns discovery and four co-location patterns mining algorithms proposed in recent years are analyzed, and a new join-less approach for co-location patterns mining, which based on a data structure----iCPI-tree (Improved Co-location Pattern Instance Tree), is proposed. The iCPI-tree is an improved version of the CPI-tree which materializes spatial neighbor relationships in order to acceler-ate the process of identifying co-location instances. This paper proves the correctness and completeness of the new ap-proach. Finally, an experimental evaluations using synthetic and real world datasets show that the algorithm is computa-tionally more efficient.},
}

@inproceedings{Yoo2004A,
  title={A partial join approach for mining co-location patterns},
  author={Yoo, Jin Soung and Shekhar, Shashi and Smith, John and Kumquat, Julius P},
  booktitle={ACM International Workshop on Geographic Information Systems, Acm-Gis 2004, November 12-13, 2004, Washington, Dc, Usa, Proceedings},
  pages={241-249},
  year={2004},
 keywords={association rule;co-location;join;spatial data mining},
 abstract={ABSTRACT  Spatial co-location patterns represent the subsets of events whose instances are frequently located together in geographic space. We identified the computational bottleneck in the execution time of a current co-location mining algorithm. A large fraction of the join-based co-location miner algorithm is devoted to computing joins to identify instances of candidate co-location patterns. We propose a novel partial-join approach for mining co-location patterns efficiently. It transactionizes continuous spatial data while keeping track of the spatial information not modeled by transactions. It uses a transaction-based Apriori algorithm as a building block and adopts the instance join method for residual instances not identified in transactions. We show that the algorithm is correct and complete in finding all co-location rules which have prevalence and conditional probability above the given thresholds. An experimental evaluation using synthetic datasets and a real dataset shows that our algorithm is computationally more efficient than the join-based algorithm.},
}

@phdthesis{李文栋2015基于,
  title={基于Spark的大数据挖掘技术的研究与实现},
  author={李文栋},
  school={山东大学},
  year={2015},
  address={济南},
 keywords={大数据挖掘技术;立体式平台;分布式集群;协同过滤推荐算法;Spark并行计算框架},
 abstract={大数据和大数据分析是目前IT领域里最炙手可热的概念,大数据具有3V的特点:(1)Volume:数据规模大;(2)Velocity:处理要求快,实时性要求比较高;(3)Variety:数据有丰富的多样性。为了让数据产生更大的价值,就需要选择一个高效的大数据处理平台来对其进行相应的分析。Spark是适用于大数据的高可靠性、高性能分布式并行计算框架。目前在国内外,已经有很多公司在实际生产环境中广泛使用Spark,比如国外的谷歌、亚马逊,易贝、雅虎等公司和国内的淘宝,百度,华为,优酷土豆等公司。　　尽管Spark在实际的工业生产和实践中已经有了广泛的使用,但是受限于其诞生时...},
}

@inproceedings{Zaharia2010Spark,
  title={Spark: cluster computing with working sets},
  author={Zaharia, Matei and Chowdhury, Mosharaf and Franklin, Michael J and Shenker, Scott and Stoica, Ion},
  booktitle={Usenix Conference on Hot Topics in Cloud Computing},
  pages={10-10},
  year={2010},
 keywords={elliptic systems;annular domains;positive radial solutions;fixed points;topological degree},
 abstract={MapReduce and its variants have been highly successful in implementing large-scale data intensive applications on clusters of unreliable machines. However, most of these systems are built around an acyclic data flow programming model that is not suitable for other popular applications. In this paper, we focus on one such class of applications: those that reuse a working set of data across multiple parallel operations. This includes many iterative machine learning algorithms, as well as interactive data analysis environments. We propose a new framework called Spark that supports these applications while maintaining the scalability and fault-tolerance properties of MapReduce. To achieve these goals, Spark introduces a data abstraction called resilient distributed datasets (RDDs). An RDD is a read-only collection of objects partitioned across a set of machines that can be rebuilt if a partition is lost. Spark can outperform Hadoop by 10x in iterative machine learning jobs, and can be used to interactively query a 39 GB dataset with sub-second response time.},
}

@inproceedings{Biau2003Spatial,
	Author = {Biau, G},
	Booktitle = {Mathematical Methods of Statistics},
	Keywords = {CiteSeerX;citations;Spatial kernel density estimation;G Biau},
	Pages = {371-390},
	Title = {Spatial kernel density estimation},
	Year = {2003}}

@article{Kloog2009Using,
	Abstract = {The kernel density (KD) function estimates the intensity of events across a surface by calculating the overall number of cases situated within a given search radius from a target point. To form a continuous surface from individual observations, the KD technique does not require the presence of a parameter鈥檚 value in a given location (e.g., the incidence rate of a disease). This feature of KD smoothing is especially beneficial for empirical studies in which individual observations are represented by geographic coordinates only and have no other attributes, required by more commonly used smoothing techniques, such as spline and kriging. In the present study, we illustrate the use of KD technique for a study of association between the geographical distributions of breast cancer cases and exposure to artificial illumination during nighttime (light-at-night or LAN) in the city of Haifa, Israel.},
	Author = {Kloog, Itai and Haim, Abraham and Portnov, Boris A.},
	Journal = {Computers Environment \& Urban Systems},
	Keywords = {Kernel density function;Light-at-night;Breast cancer;GIS},
	Number = {1},
	Pages = {55-63},
	Title = {Using kernel density function as an urban analysis tool: Investigating the association between nightlight exposure and the incidence of breast cancer in Haifa, Israel},
	Volume = {33},
	Year = {2009}}

@article{禹文豪网络核密度,
	Abstract = {城市空间POI点的分布模式、分布密度在基础设施规划、城市空间分析中具有重要意义,表达该特征的核密度法(kernel density estimation)由于顾及了地理学第一定律的区位影响,比其他密度表达方法(如样方密度、基于V or onoi图密度)占优。然而,传统的核密度计算方法往往基于二维延展的欧氏空间,忽略了城市网络空间中设施点的服务功能及相互联系发生于网络路径距离而非欧氏距离的事实。本研究针对该缺陷,给出了网络空间核密度计算模型,分析了核密度方法在置入网络结构中受多种约束条件的扩展模式,讨论了衰减阈值及高度极值对核密度特征表达的影响。通过实际多种POI点分布模式(随机型、稀疏型、区域密集型、线状密集型)下的核密度分析试验,讨论了POI基础设施在城市区域中的分布特征、影响因素、服务功能。},
	Author = {禹文豪 and 艾廷华},
	Journal = {测绘学报},
	Keywords = {网络核密度;POI点分析;网络分析;空间统计},
	Number = {1},
	Pages = {82-90},
	Title = {核密度估计法支持下的网络空间POI点可视化与分析},
	Volume = {44},
	Year = {2015}}

@article{甄峰网络社会,
	Abstract = {摘　要: 信息技术影响下的城市区域空间结构变化得到了国内外学者的关注。本文以新浪微博为例,从网络社会空间的角度人手,对中国城市网络发展特征进行了研究。研究表明:微博社会空间视角下的中国城市网络存在着明显的等级关系与层级区分,城市的网络连接度与城市等级表现出了相对一致性。根据城市网络层级与网络联系强度,东部、中部、西部3大区域板块的网络联系差异明显,东部地区内部的联系,以及东部与中部地区和西部地区的联系几乎构成当前网络体系中的全部。城市网络呈现出分层集聚现象,具体表现为``三大四小''发展格局,即京津冀区域、珠三角区域、长三角区域、成渝地区、海西地区、武汉地区、东北地区。高等级城市在整个城市网络中处于绝对支配地位,北京以突出的优势成为全国性的网络联系中心,而上海、广州、深圳则成为全国性的网络联系副中心。},
	Author = {甄峰 and 王波 and 陈映雪},
	Journal = {地理学报},
	Keywords = {网络社会空间;新浪微博;网络连接度;中国城市网络;特征},
	Number = {8},
	Pages = {1031-1043},
	Title = {基于网络社会空间的中国城市网络特征------以新浪微博为例},
	Volume = {67},
	Year = {2012}}

@inproceedings{曹盼盼全国,
	Abstract = {人类行为和社会环境是相互作用的,定量研究人类行为的宏观规律有助于我们理解社会这个复杂网络。我们期望通过对记录人类活动历史的数据库的研究中挖掘出了人类行为的统计规律。本文从书信入手,对中国名人的书信时间进行统计,探索人类书信间隔时间的统计规律,分析其所服从的幂律分布和齐普夫定律,并论证幂律分布和齐普夫定律之间的关系,揭示出幂律特性在人类社会行为中的普适性。},
	Author = {曹盼盼 and 阎春宁},
	Booktitle = {全国复杂网络学术会议论文},
	Keywords = {人类行为;间隔时间;幂律分布;齐普夫定律},
	Pages = {51-56},
  address={青岛},
	Title = {人类通信模式的幂律分布和Zipf定律},
	Year = {2009}}

@article{Boldi2009PageRank,
	Author = {Boldi, Paolo and Santini, Massimo and Vigna, Sebastiano},
	Journal = {Acm Transactions on Information Systems},
	Number = {4},
	Pages = {1-23},
	Title = {PageRank},
	Volume = {27},
	Year = {2009}}

@article{冯梅陈鹏钢铁,
	Abstract = {本文运用综合指数法并结合灰色系统理论,量化分析了中国钢铁产业1996-2012年产能过剩程度,对未来三年钢铁产业产能过剩程度进行了预警,并从政府、行业协会和企业角度分别提出可操作性的政策建议。研究表明,中国钢铁产业产能过剩的问题长期存在,随着国内外经济环境变化,有可能进一步加剧,化解产能过剩矛盾迫在眉睫。},
	Author = {冯梅 and 陈鹏},
	Journal = {中国软科学},
	Keywords = {钢铁产业;产能过剩;综合指数;灰色预警},
	Number = {5},
	Pages = {110-116},
	Title = {中国钢铁产业产能过剩程度的量化分析与预警},
	Year = {2013}}

@article{Girvan2002Community,
	Abstract = {A number of recent studies have focused on the statistical properties of networked systems such as social networks and the World-Wide Web. Researchers have concentrated particularly on a few properties which seem to be common to many networks: the small-world property, power-law degree distributions, and network transitivity. In this paper, we highlight another property which is found in many networks, the property of community structure, in which network nodes are joined together in tightly-knit groups between which there are only looser connections. We propose a new method for detecting such communities, built around the idea of using centrality indices to find community boundaries. We test our method on computer generated and real-world graphs whose community structure is already known, and find that it detects this known structure with high sensitivity and reliability. We also apply the method to two networks whose community structure is not well-known - a collaboration network and a food web - and find that it detects significant and informative community divisions in both cases.},
	Author = {Girvan, M and Newman, M. E.},
	Journal = {Proceedings of the National Academy of Sciences of the United States of America},
	Keywords = {Applied Mathematics},
	Number = {12},
	Pages = {7821},
	Title = {Community structure in social and biological networks},
	Volume = {99},
	Year = {2002}}

@article{Newman2012Communities,
	Abstract = {Networks, also called graphs by mathematicians, provide a useful abstraction of the structure of many complex systems, ranging from social systems and computer networks to biological networks and the state spaces of physical systems. In the past decade there have been significant advances in experiments to determine the topological structure of networked systems, but there remain substantial challenges in extracting scientific understanding from the large quantities of data produced by the experiments. A variety of basic measures and metrics are available that can tell us about small-scale structure in networks, such as correlations, connections and recurrent patterns, but it is considerably more difficult to quantify structure on medium and large scales, to understand the `big picture'. Important progress has been made, however, within the past few years, a selection of which is reviewed here.},
	Author = {Newman, M. E. J.},
	Journal = {Nature Physics},
	Keywords = {Statistical physics, thermodynamics and nonlinear dynamics},
	Number = {8},
	Pages = {25-31},
	Title = {Communities, modules and large-scale structure in networks},
	Volume = {8},
	Year = {2012}}

@inproceedings{Gonzalez2014GraphX,
	Abstract = {In pursuit of graph processing performance, the systems community has largely abandoned general-purpose dis- tributed dataflow frameworks in favor of specialized graph processing systems that provide tailored programming ab- stractions and accelerate the execution of iterative graph algorithms. In this paper we argue that many of the advan- tages of specialized graph processing systems can be re- covered in a modern general-purpose distributed dataflow system. We introduce GraphX, an embedded graph pro- cessing framework built on top of Apache Spark, a widely used distributed dataflow system. GraphX presents a fa- miliar composable graph abstraction that is sufficient to express existing graph APIs, yet can be implemented us- ing only a few basic dataflow operators (e.g., join, map, group-by). To achieve performance parity with special- ized graph systems, GraphX recasts graph-specific op- timizations as distributed join optimizations and mate- rialized view maintenance. By leveraging advances in distributed dataflow frameworks, GraphX brings low-cost fault tolerance to graph processing. We evaluate GraphX on real workloads and demonstrate that GraphX achieves an order of magnitude performance gain over the base dataflow framework and matches the performance of spe- cialized graph processing systems while enabling a wider range of computation.},
	Author = {Gonzalez, Joseph E and Xin, Reynold S and Dave, Ankur and Crankshaw, Daniel and Franklin, Michael J and Stoica, Ion},
	Booktitle = {Usenix Conference on Operating Systems Design and Implementation},
	Pages = {599-613},
	Title = {GraphX: graph processing in a distributed dataflow framework},
	Year = {2014}}

@article{杨小柳乡土中国,
	Abstract = {本文从乡土中国的研究脉络出 发,以改革开放以来人口迁移所致的城市性质的变迁为表述线索,指出我国城乡社会在政策引导下正在经历着从以乡村为中心的地域城市体系阶段到以城市为中心的 移民城市体系阶段的内源性的巨大转交。从地域城市到移民城市的转交,不仅指我国城市在人口结构、文化景观、城市功能等方面的转型,更意味着我国从乡土社会 向全国性城市社会的转变。},
	Author = {杨小柳},
	Date-Modified = {2017-03-22 14:15:39 +0000},
	Journal = {民族研究},
	Keywords = {移民城市;地域城市;人口迁移},
	Number = {5},
	Pages = {41-51},
	Title = {从地域城市到移民城市:全国性城市社会的构建},
	Year = {2015}}

@article{Andrienko2013Thematic,
  title={Thematic Patterns in Georeferenced Tweets through Space-Time Visual Analytics},
  author={Andrienko, Gennady and Andrienko, Natalia and Bosch, Harald and Ertl, Thomas and Fuchs, Georg and Jankowski, Piotr and Thom, Dennis},
  journal={Computing in Science \& Engineering},
  volume={15},
  number={3},
  pages={72-82},
  year={2013},
 keywords={data analysis;data visualisation;knowledge acquisition;social networking (online;Seattle-area residents;georeferenced Twitter data;knowledge extraction;space-time visual analytics;thematic patterns;Analytical models},
 abstract={An exploratory study of the potential of georeferenced Twitter data (using tweets from Seattle-area residents over a two-month period) extracts knowledge about people's everyday life.},
}

@article{Linna2013Spatial,
  title={Spatial, temporal, and socioeconomic patterns in the use of Twitter and Flickr},
  author={Linna   Li and Michael F.   Goodchild and Bo   Xu},
  journal={Cartography and Geographic Information Science},
  volume={40},
  number={2},
  pages={61-77},
  year={2013},
 keywords={INFORMATION},
 abstract={Introduction    There has been a rapid expansion in the use of social media and data sharing...},
}

@article{Hollenstein2010Exploring,
  title={Exploring place through user-generated content: Using Flickr to describe city cores},
  author={Hollenstein, Livia and Purves, Ross},
  journal={Journal of Spatial Information Science},
  volume={1},
  number={1},
  pages={21-48},
  year={2010},
 keywords={user-generated content;city core;vernacular geography;gazetteers;georeferenced;tagging;Flickr},
 abstract={Terms used to describe city centers, such as Downtown, are key concepts in everyday or vernacular language. Here, we explore such language by harvesting georeferenced and tagged metadata associated with 8 million Flickr images and thus consider how large numbers of people name city core areas. The nature of errors and imprecision in tagging and georeferencing are quantified, and automatically generated precision measures appear to mirror errors in the positioning of images. Users seek to ascribe appropriate semantics to images, though bulk-uploading and bulk-tagging may introduce bias. Between 0.5-2% of tags associated with georeferenced images analyzed describe city core areas generically, while 70% of all georeferenced images analyzed include specific place name tags, with place names at the granularity of city names being by far the most common. Using Flickr metadata, it is possible not only to describe the use of the term Downtown across the USA, but also to explore the borders of city center neighborhoods at the level of individual cities, whilst accounting for bias by the use of tag profiles.},
}

@article{Liu2015The,
  title={The geography of Weibo},
  author={Liu, Xingjian and Wang, Jianghao},
  journal={Environment \& Planning A},
  volume={47},
  number={6},
  pages={1231-1234},
  year={2015},
 keywords={Article},
 abstract={No abstract is available for this item.},
}

@article{刘大均2015中国旅游微博空间分布格局及影响因素,
  title={中国旅游微博空间分布格局及影响因素--以新浪旅游微博为例},
  author={刘大均 and 胡静 and 程绍文 and 陈君子 and 张琪},
  journal={地理科学},
  volume={35},
  number={6},
  pages={717-724},
  year={2015},
 keywords={旅游微博;空间格局;新浪旅游微博;中国},
 abstract={运用空间分析法、位序-规模法则、多元回归等方法,对中国旅游微博的空间分布格局进行分析,并揭示旅游微博空间分布的影响因素。研究表明:①中国旅游微博 以政府旅游微博为主体,旅游专业网站微博为重要补充,旅游景区、旅游协会微博所占比重较小。②旅游微博东密西疏的梯度分布格局较为突出,省际分布集中性 强,且大多分布在人口规模大、行政级别高的城市。③旅游微博规模分布满足齐夫法则,双分形结构明显,但不同类型旅游微博规模等级结构以及发育程度差异较 大。④旅游微博空间分布受人口规模、信息化程度、旅游资源禀赋等因素的综合影响,不同类型旅游微博空间分布的影响因素表现出一定的差异性。},
}

@article{徐艳2014基于新浪微博视角下的城市网络空间特征分析——以重庆市主城区为例,
  title={基于新浪微博视角下的城市网络空间特征分析——以重庆市主城区为例},
  author={徐艳 and 黎明},
  journal={西南师范大学学报(自然科学版)},
  volume={39},
  number={6},
  pages={43-49},
  year={2014},
 keywords={网络空间;新浪微博;网络连接度;城市网络;特征;重庆},
 abstract={城市空间结构一直受到各界学者的广泛关注.尤其在信息高速发展的时代,信息技术影响下的城市空间结构更是发生了巨大的变化.从城市网络空间的概念入手,以新浪微博为切入点,对重庆市主城区的城市网络结构特征进行了研究.从研究中得知城市网络存在明显的层级关系,以新浪微博的统计数据为依据,重庆市各区之间呈现出不同的比重与层级:所算出的网络连接度与传统地理学上城市各区域之间的层级表现出相对一致性,同时又表现出与传统意义上的层级稍有差异的情况.为今后城市规划相关部门提供了一个高效快速的分析方式及决策参考标准.},
}

@article{禹文豪2015设施,
  title={设施POI的局部空间同位模式挖掘及范围界定},
  author={禹文豪 and 艾廷华 and 周启},
  journal={地理与地理信息科学},
  volume={31},
  number={4},
  pages={6-11},
  year={2015},
}

@article{Blondel2008Fast,
  title={Fast unfolding of communities in large networks},
  author={Blondel, Vincent D and Guillaume, Jean Loup and Lambiotte, Renaud and Lefebvre, Etienne},
  journal={Journal of Statistical Mechanics Theory \& Experiment},
  volume={2008},
  number={10},
  pages={155-168},
  year={2008},
}

@article{周蔷2017微博的话语权分配研究——以新浪微博风云人物榜为个案,
  title={微博的话语权分配研究——以新浪微博风云人物榜为个案},
  author={周蔷},
  journal={资源信息与工程},
  volume={32},
  number={1},
  year={2017},
}

@article{李小文2007地理学第一定律与时空邻近度的提出,
  title={地理学第一定律与时空邻近度的提出},
  author={李小文 and 曹春香 and 常超一},
  journal={自然杂志},
  volume={29},
  number={2},
  pages={69-71},
  year={2007},
}

@article{Girvan2001Community,
  title={Community structure in social and biological networks},
  author={Girvan, M. and Newman, M. E. J.},
  journal={Proceedings of the National Academy of Sciences of the United States of America},
  volume={99},
  number={12},
  pages={7821},
  year={2001},
}

@article{Raghavan2007Near,
  title={Near linear time algorithm to detect community structures in large-scale networks.},
  author={Raghavan, U. N. and Albert, R and Kumara, S},
  journal={Physical Review E Statistical Nonlinear \& Soft Matter Physics},
  volume={76},
  number={2},
  pages={036106},
  year={2007},
}

@article{Newman2006Modularity,
  title={Modularity and community structure in networks},
  author={Newman, M. E.},
  journal={Proceedings of the National Academy of Sciences},
  volume={103},
  number={23},
  pages={8577},
  year={2006},
}

@book{阿尔温1992权力的转移,
  title={权力的转移},
  author={阿尔温.托夫勒},
  publisher={中共中央党校出版社},
  year={1992},
  address={北京},
}

@article{李德仁2013智慧地球时代测绘地理信息学的新使命,
  title={智慧地球时代测绘地理信息学的新使命},
  author={李德仁},
  journal={中国测绘},
  volume={37},
  number={1},
  pages={32-33},
  year={2013},
}